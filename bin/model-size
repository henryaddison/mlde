#!/usr/bin/env python
# calculate the number of parameters in a model

from pathlib import Path
import torchinfo
from ml_collections import config_dict

import typer
import logging
import yaml

from ml_downscaling_emulator.models.location_params import (
    LocationParams,
)

from ml_downscaling_emulator.models import utils as mutils

from ml_downscaling_emulator.models import cncsnpp  # noqa: F401
from ml_downscaling_emulator.models import cunet  # noqa: F401

from ml_downscaling_emulator.models import (  # noqa: F401
    layerspp,  # noqa: F401
)  # noqa: F401
from ml_downscaling_emulator.models import layers  # noqa: F401
from ml_downscaling_emulator.models import (  # noqa: F401
    normalization,  # noqa: F401
)  # noqa: F401

from ml_downscaling_emulator.utils import model_size, param_count

logger = logging.getLogger()
logger.setLevel("INFO")

app = typer.Typer()


def load_model(config):
    logger.info(f"Loading model from config")
    score_model = mutils.get_model(config.model.name)(config)
    location_params = LocationParams(
        config.model.loc_spec_channels, config.data.image_size
    )

    return score_model, location_params


def load_yml_config(config_path):
    with open(config_path) as f:
        config = config_dict.ConfigDict(yaml.unsafe_load(f))

    return config


def load_py_config(config_path):
    import importlib.util

    spec = importlib.util.spec_from_file_location("config_module", config_path)
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    return config_module.get_config()


def load_config(config_path: Path):
    logger.info(f"Loading config from {config_path}")
    if config_path.suffix == ".yml":
        return load_yml_config(config_path)
    elif config_path.suffix == ".py":
        return load_py_config(config_path)
    else:
        raise ValueError(f"Unsupported config format: {config_path}")


@app.command()
def main(
    config_path: Path,
    depth: int = 2,
):
    if config_path.is_dir():  # assume a workdir with a config.yml file
        config_path = config_path / "config.yml"

    config = load_config(config_path)
    score_model, location_params = load_model(config)

    num_score_model_parameters = param_count(score_model)
    num_location_parameters = param_count(location_params)

    typer.echo(f"Score model has {num_score_model_parameters} parameters")
    typer.echo(f"Location parameters have {num_location_parameters} parameters")

    size_all_mb = sum(
        model_size(model)
        for model in [
            score_model,
            location_params,
        ]
    )

    typer.echo("model size: {:.3f}MB".format(size_all_mb))

    torchinfo.summary(
        score_model, depth=depth
    )  # , col_names=["input_size", "output_size",], input_size=[(16, 1, 64, 64), (16, 13, 64, 64), (1,)])


if __name__ == "__main__":
    app()
